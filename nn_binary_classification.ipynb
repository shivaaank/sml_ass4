{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd5d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import struct\n",
    "from array import array\n",
    "import random\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "n_samples_per_class = 10\n",
    "mean0 = np.array([-1, -1])\n",
    "mean1 = np.array([1, 1])\n",
    "cov = np.eye(2)\n",
    "\n",
    "X0 = np.random.multivariate_normal(mean0, cov, n_samples_per_class).T\n",
    "X1 = np.random.multivariate_normal(mean1, cov, n_samples_per_class).T\n",
    "# print(X0.shape) #each col is a sample\n",
    "Y0 = np.zeros((1, n_samples_per_class))\n",
    "Y1 = np.ones((1, n_samples_per_class))\n",
    "# Combine and shuffle\n",
    "X = np.concatenate([X0, X1], axis=1)\n",
    "Y = np.concatenate([Y0, Y1], axis=1)\n",
    "perm = np.random.permutation(2 * n_samples_per_class)\n",
    "X, Y = X[:, perm], Y[:, perm]\n",
    "split_idx = n_samples_per_class  # 20 samples total => 10 train, 10 test\n",
    "X_train, X_test = X[:, :split_idx], X[:, split_idx:]\n",
    "Y_train, Y_test = Y[:, :split_idx], Y[:, split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502447fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_dim=2, hidden_dim=1, output_dim=1, lr=0.1):\n",
    "        self.w1 = np.random.randn(hidden_dim, input_dim)\n",
    "        self.b1 = np.random.randn(hidden_dim, 1)\n",
    "        self.w2 = np.random.randn(output_dim, hidden_dim)\n",
    "        self.b2 = np.random.randn(output_dim, 1)\n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def l2_derivative(self, F, Y):\n",
    "        return 2*(Y-F)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape: (input_dim, batch_size)\n",
    "        self.y1 = self.w1.dot(X) + self.b1       # (hidden_dim, batch_size)\n",
    "        self.z1 = self.sigmoid(self.y1)\n",
    "        self.y2 = self.w2.dot(self.z1) + self.b2 # (output_dim, batch_size)\n",
    "        self.z2 = self.y2  # No activation on output\n",
    "        return self.z2\n",
    "\n",
    "    def compute_loss(self, F, Y):\n",
    "        return np.mean((F - Y) ** 2)\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "        # Gradient of loss wrt z2\n",
    "        dz2 = -self.l2_derivative(self.z2, Y)/m\n",
    "        # dy2 = 2 * (self.z2 - Y) / m              # (1, m)\n",
    "        dy2 = dz2                                # Since output is linear\n",
    "        dw2 = dy2.dot(self.z1.T)                # (1, hidden_dim)\n",
    "        db2 = np.sum(dy2, axis=1, keepdims=True)\n",
    "\n",
    "        dz1 = self.w2.T.dot(dy2)                # (hidden_dim, m)\n",
    "        dy1 = dz1 * self.sigmoid_derivative(self.y1)\n",
    "        dw1 = dy1.dot(X.T)                      # (hidden_dim, input_dim)\n",
    "        db1 = np.sum(dy1, axis=1, keepdims=True)\n",
    "\n",
    "        # Update parameters\n",
    "        self.w2 -= self.lr * dw2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.w1 -= self.lr * dw1\n",
    "        self.b1 -= self.lr * db1\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs=1000):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            F = self.forward(X_train)\n",
    "            loss = self.compute_loss(F, Y_train)\n",
    "            self.backward(X_train, Y_train)\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d716d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5828120831282753\n",
      "Epoch 2/10, Loss: 0.3689730598365617\n",
      "Epoch 3/10, Loss: 0.25356333213746896\n",
      "Epoch 4/10, Loss: 0.1903801858190322\n",
      "Epoch 5/10, Loss: 0.15494561500174678\n",
      "Epoch 6/10, Loss: 0.1342825210515619\n",
      "Epoch 7/10, Loss: 0.12151048033145062\n",
      "Epoch 8/10, Loss: 0.11298177961612324\n",
      "Epoch 9/10, Loss: 0.1067624544743682\n",
      "Epoch 10/10, Loss: 0.10182670144784414\n",
      "Test MSE: 0.1698068526856277\n"
     ]
    }
   ],
   "source": [
    "nn = SimpleNN(lr=0.1)\n",
    "nn.train(X_train, Y_train, epochs=10)\n",
    "\n",
    "# Compute test MSE\n",
    "Y_pred_test = nn.forward(X_test)\n",
    "test_mse = nn.compute_loss(Y_pred_test, Y_test)\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
